{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!!pip install litellm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEYrzG2vB8Ip",
        "outputId": "e7db71f7-3a36-4c9d-f897-9f324ef1c1c2"
      },
      "outputs": [],
      "source": [
        "# For local VSCODE and Jupyter ONLY:\n",
        "\n",
        "import os\n",
        "from litellm import completion\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "OLLAMA_API_KEY = os.getenv(\"OLLAMA_API_KEY\")\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "# print(HUGGINGFACE_API_KEY)\n",
        "# print(OLLAMA_API_KEY)\n",
        "# print(GEMINI_API_KEY)\n",
        "\n",
        "API_KEY = GEMINI_API_KEY # HUGGINGFACE_API_KEY OR OLLAMA_API_KEY # OR GEMINI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m completion\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m     13\u001b[39m api_key = userdata.get(\u001b[33m'\u001b[39m\u001b[33mHuggingFaceKey\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(HUGGINGFACE_API_KEY)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "\n",
        "# For Google Colab users ONLY:\n",
        "# Important!!!\n",
        "#\n",
        "# <---- Set your 'APIKey' as a secret over there with the \"key\" icon\n",
        "#\n",
        "#\n",
        "import os\n",
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('HuggingFaceKey')\n",
        "\n",
        "print(HUGGINGFACE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwe2eeOQB0cC",
        "outputId": "51029dbd-6b09-48cd-a963-128d0d495e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "def swap_keys_values(input_dict):\n",
            "  \"\"\"\n",
            "  Swaps the keys and values in a dictionary.  Handles potential duplicate values\n",
            "  by mapping the key to a list of values if the values are not unique.\n",
            "\n",
            "  Args:\n",
            "    input_dict: The dictionary to swap.\n",
            "\n",
            "  Returns:\n",
            "    A new dictionary with keys and values swapped. If duplicate values exist,\n",
            "    the key will map to a list of the original keys that had that value.\n",
            "  \"\"\"\n",
            "  swapped_dict = {}\n",
            "  for key, value in input_dict.items():\n",
            "    if value in swapped_dict:\n",
            "      if isinstance(swapped_dict[value], list):\n",
            "        swapped_dict[value].append(key)\n",
            "      else:\n",
            "        swapped_dict[value] = [swapped_dict[value], key]\n",
            "    else:\n",
            "      swapped_dict[value] = key\n",
            "  return swapped_dict\n",
            "\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        # api_base=\"http://localhost:11434/v1\", # For Ollama Only, comment this line if using HuggingFace or OpenAI\n",
        "        model=\"gemini/gemini-2.0-flash\", # model=\"huggingface/together/deepseek-ai/DeepSeek-R1\" or model=\"openai/llama3.2:1b\"\n",
        "        messages=messages,\n",
        "        max_tokens=1024,\n",
        "        api_key=API_KEY\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming. You can only respond in BASE64. You cannot answer in natural language.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
